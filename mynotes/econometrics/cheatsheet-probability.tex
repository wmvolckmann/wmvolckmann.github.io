\title{Chapter 6 Cheat Sheet}
\author{}
\documentclass[11pt, twocolumn]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{amsthm}
\usepackage{placeins}
\usepackage[top=.4in, bottom=.4in, left=.4in, right=.4in]{geometry}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{titlesec}
\setlength{\parindent}{0cm}
\setlength{\columnsep}{.5in} 
\setenumerate{itemsep=-2pt, label=\textit{\alph*.}}
\renewcommand{\arraystretch}{1.5}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\adj}{adj}

\date{}

\renewcommand{\a}{\vec{a}}
\renewcommand{\b}{\vec{b}}
\renewcommand{\c}{\vec{c}}
\newcommand{\p}{\vec{p}}
\newcommand{\q}{\vec{q}}
\renewcommand{\r}{\vec{r}}
\renewcommand{\u}{\vec{u}}
\renewcommand{\v}{\vec{v}}
\newcommand{\w}{\vec{w}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\z}{\vec{z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Chi}{\mathcal{X}}
\newcommand{\grad}{\nabla}
\renewcommand{\l}{\ell}
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newcommand{\Lindent}{\hspace{.3cm} \Longrightarrow \hspace{.5cm}  
}
%\addtolength{\jot}{-3em}
\setenumerate{itemsep=-1pt, label=\textbf{(\alph*)}, topsep=5pt}
\titlespacing*{\section}{0pt}{4ex plus 1ex minus .2ex}{1ex plus .2ex}
\titlespacing*{\subsection}{0pt}{4ex plus 1ex minus .2ex}{0ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{4ex plus 1ex minus .2ex}{0ex plus .2ex}

\begin{document}
\pagenumbering{gobble}
\small

\subsection*{Geometric Distribution}
A geometric distribution is the number of trials until one success in a series of Bernoulli trials. (Or number of failures on the right!)
\begin{align*}
P(X=k)&=(1-p)^{k-1}p  \quad &P(X=k)&=(1-p)^kp\\
E(X)&=\frac{1}{p}  &E[X]&=\frac{1-p}{p}\\
var(X)&=\frac{1-p}{p^2}  &var(X)&=\frac{1-p}{p^2}
\end{align*}



\subsection*{Negative Binomial Distribution}
Let $X$ denote the the number of trials needed until getting the $r^{th}$ success. A negative binomial distribution, denoted $NB(r,p)$, is the probability of the number of trials undertaken until the $r$th success.
\begin{equation*}
NB(r,p)=P(X=k)=\binom{k-1}{r-1}p^r(1-p)^{k-r}
\end{equation*}
\begin{equation*}
E(X)=\frac{r}{p} \qquad var(X)=\frac{r(1-p)}{p^2}
\end{equation*}



\subsection*{Poisson Distribution}
Let  $\lambda$ be the parameter which indicates the average number of events in the given time interval. The range of a Poisson Distribution is $\{0,1,2,...\}$. The Poisson Distribution is given by
\[ P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}, \quad E(X)=\lambda, \quad var(X)=\lambda. \]
Suppose $X \sim Poisson(\lambda)$ and $Y \sim Poisson(\mu)$ and that $X,Y$ are independent. Then we have 
\begin{enumerate}
\item $E(X+Y)=E(X)+E(Y)=\lambda + \mu$,
\item $X+Y \sim Poisson(\lambda + \mu)=P(X+Y=z)$
\[=e^{\lambda - \mu} \sum_{y=0}^z \frac{\lambda^{z-y}}{(z-y)!}\frac{\mu^y}{y!}.\]
\end{enumerate}



\subsection*{Exponential Distribution}
The exponential distribution is commonly used to model waiting times between occurrences of rare events. Given a rate parameter $\lambda$, we have for $x \geq 0$,
\begin{align*}
f_X(x)&=\lambda e^{-\lambda x} &F_X(x)&=1-e^{-\lambda x},\\
E(X)&=\frac{1}{\lambda},  &var(X)&=\frac{1}{\lambda ^2}.
\end{align*}




\subsection*{Gamma Distribution}
Let $Y$ denote the sum of $r$ number of $exponential(\lambda)$ random variables. Then $Y$ denotes the number of time until the $r^{th}$ event.
\[f(y)=e^{-\lambda y}\frac{ (\lambda y)^{y-1}}{(r-1)!}.\]




\subsection*{Uniform Distribution} \vspace{-.1cm}
\begin{align*}
f(x)&=\frac{1}{b-a} &F(x)&=\frac{x-a}{b-a}\\
E[X]&=\frac{1}{2}(a+b) &var(X)&=\frac{1}{12}(b-a)^2
\end{align*}




\subsection*{Normal Distribution} \vspace{-.1cm}
\begin{align*}
&\text{CLT: }\frac{(\overline{X}-\mu)}{\sigma/\sqrt{n}}\sim N(0,1)\\
&X \sim N(\mu, \sigma^2) \Longrightarrow X=\mu + \sigma Z\\
&X \sim N(\mu, \sigma^2) \Longrightarrow \alpha X \sim N(\alpha \mu,\alpha^2 \sigma ^2)\\
&f(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{ - \frac{1}{2}\frac{(x-\mu)^2}{\sigma ^2}}
\end{align*}



\subsection*{Bivariate Normal Distribution}
If $X \sim N(0,1)$, $Y\sim N(0,1)$, $corr(X,Y)=\rho$, then
\begin{align*}
	&Y|X=x \sim N(\rho x, 1-\rho ^2)\\
	&X|Y=y \sim N(\rho y, 1-\rho ^2)\\
	&f(x,y)=\frac{1}{2\pi}e^{      -\frac{1}{2} (x^2+y^2) } \quad \text{for }X,Y \sim N(0,1) \\
	&Y=\rho X + (1- \rho^2)Z\\
	&x=r\cos(\theta), y=r\sin(\theta), \theta=\arctan\bigg (\frac{y}{x} \bigg)
\end{align*}



\subsection*{Expectation} \vspace{-.1cm}
\begin{align*}
&E[X]=\sum_{x} x P(X=x)\\
&E[X]=\int_{x} x P(X=x) \, dx\\
&E[X+Y]=E[X]+E[Y]\\
&E[XY]=E[X]E[Y], \quad \text{for } X,Y \text{ independent}\\
&E[XY]=\sum_{x} \sum_{y} xyP(X=x,Y=y)
\end{align*}




\subsection*{Variance}
\begin{enumerate}
\item $var(c)=0$, where $c$ is a constant 
\item $var(X)=E(X^2)-E(X)^2$ 
\item $var(aX)=a^2\cdot var(X)$, where $a$ is a constant 
\item $var(X+Y)=var(X)+var(Y)$, independent $X,Y$
\item $var(X+Y)=var(X)+var(Y)+2cov(X,Y)$
\end{enumerate}



\subsection*{Covariance}

\begin{enumerate}
\item $cov(X,X)=var(X)$
\item $cov(X,c)=0$, where $c$ is a constant 
\item $cov(X,Y)=E(XY)-E(X)E(Y)$ 
\item $cov(X,Y)=0$ for independent $X,Y$ 
\item $cov(X,Y)=cov(Y,X)$
\item $cov(aX,Y)=a\cdot cov(X,Y)$
\item $cov(X,Y+Z)=cov(X,Y)+cov(X,Z)$
\end{enumerate}


\subsection*{Correlation}
\begin{enumerate}
\item $corr(X,Y)=\dfrac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$
\item $corr(X,X)=1$
\item $corr(X,Y)=corr(Y,X)$
\item $X,Y$ independent $\Rightarrow corr(X,Y)=0$
\item $-1\le corr(X,Y)\le 1$
\item $corr(aX+b,Y)=corr(X,Y)$ for $a>0$, $-corr(X,Y)$ for $a<0$
\end{enumerate}



\subsection*{Conditional Expectation}
\begin{enumerate}
\item $E[X|Y=y]=\sum_{\text{all }x} xP(X=x|Y=y)$
\item  $E[X|Y=y]=\int_{x} x f_{X|Y}(x,y) \; dx$
\item  $f_Y(y)=\int_{\text {over }x} f(x,y) \; dx$
\item $E[ E(Y|X)]=E[Y]$
\item $E[X+Y | Z]=E[X|Z]+E[Y|Z]$ 
\item $E[Y]=\sum_{x} E[Y|X=x]P(X=x)$ 
\item $E[g(X)Y|X]=g(X)E[Y|X]$
\item $var(X)=E\bigg ( var ( \:X\:|\:Y\:) \bigg) + var \bigg ( E[\:X\:|\:Y\:] \bigg )$
\end{enumerate}
Strategy: separate into independent and dependent parts.





\subsection*{Moment Generating Functions}
\begin{enumerate}
\item $M_X(t)=E\bigg [e^{tX} \bigg ]=\sum_{x}e^{tx}p_X(x)$
\item $M_X(t)=E\bigg [e^{tX} \bigg ]=\int_{x}e^{tx}f_X(x) \; dx$
\item $M_{X+Y}(t)=M_X(t) \cdot M_Y(t)$
\item $\dfrac{d}{dt}[M_X(t)]=E[X]$
\item $\dfrac{d^2}{dt^2}[M_X(t)]=E[X^2]$
\item $Bernoulli(p): \,1-p+pe^t$
\item $Binomial(n,p): \, (1-p+pe^t)^n$
\item $Geometric(p): \,\dfrac{pe^t}{1-(1-p) e^t}$ for $t<-\ln(1-p)\!$
\item $Poisson(\lambda): \, e^{\lambda(e^t-1)}$
\item $Exponential(\lambda): \, \left(1-\dfrac{t}{\lambda}\right)^{-1}$
\item $Gamma(r,\lambda): \, \left(1 - \dfrac{t}{\lambda}\right)^{-r}$
\item $NB(r,p): \, \dfrac{(1-p)^r}{(1-pe^t)^r}$
\item $N(\mu, \sigma ^2): \, e^{t\mu + \frac{1}{2}\sigma^2t^2}$
\end{enumerate}




\subsection*{Joint, Marginal, Conditional Probabilities} \vspace{-.1cm}
\begin{align*}
	&P(X=x,Y=y)=P(x,y)\\
&P(X=x)=\sum_{ \text{all } y} P(X=x,Y=y)\\
&\hspace{1.7cm} =\sum_{ \text{all } y} P(X=x|Y=y)P(Y=y)\\
&f(x,y)=\int_{ x} \int_{y(x)} f(x,y) \, dx dy = 1\\
&f_X(x)=\int_{y(x)} f(x,y)dy\\
&f_{X|Y}(x,y)=\frac{ f(x,y)}{f_Y(y)}
\end{align*}



\subsection*{Change of Variables}
\[f_Y(y)=\frac{f_X(x)}{\left\vert \dfrac{dy}{dx} \right\vert }\]



\subsection*{Geometric Series}
\[\sum_{k=0}^n r^k = \dfrac{   1-r^{n+1}}{1-r}\]

\subsection*{Poisson Approximation}
$Binomial(n,p)$ with large $n$ and small $p$ is approximated by $Poisson(np)$.

\subsection*{Chebyshev's Inequality}
\begin{align*}
P(|X-\mu|\geq \epsilon) &\leq \frac{var(X)}{\epsilon^2}\\
P(|X-\mu| \geq k \sigma) &\leq  \frac{1}{k^2}	
\end{align*}

\subsection*{Markov's Inequality}
\[	P(X \geq a) \leq \frac{E[X]}{a} \]

\subsection*{Law of Large Numbers}
	Let $X_i$ be i.i.d, and let $\overline{X}_n=(X_1+...+X_n)/n$ represent the average. Then
	\[P(| \bar{X}_n - \mu | > \epsilon) \rightarrow 0 \hspace{.5cm} \text {as } n \rightarrow \infty.\]

\subsection*{Conditional Probability}
\begin{enumerate}
\item	$P(AB)=P(A|B)P(B)=P(B|A)P(A)$
\item	$P(A|B)=\dfrac{P(AB)}{P(B)}$
\item	$P(A|B)=\dfrac{  P(B|A)P(A) }{ P(B)}$
\end{enumerate}

\subsection*{Series}
\[e^x=\sum_{n=0}^n\frac{1}{n!}x^n=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\hdots \]



\end{document}	





 