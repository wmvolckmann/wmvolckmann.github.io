\title{Dynamic Programming -- Stochastic Growth}
\author{William M Volckmann II}
\documentclass[12pt]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{xifthen}
\usepackage{titlesec}
\usepackage[normalem]{ulem}
\usepackage[final]{pdfpages}
%\usepackage[top=1.25in, left=1.25in, right=1.25in]{geometry}

\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Chi}{\mathcal{X}}
\newcommand{\grad}{\nabla}
\newcommand{\B}{\beta}
\newcommand{\BH}{\hat{\beta}}
\newcommand{\bh}{\hat{\beta}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\crit}{c_{\alpha}}
\newcommand{\given}{\; | \;}
\newcommand{\xbar}{\bar{X}_n}
\newcommand{\asim}{\overset{a}{\sim}}
\newcommand{\Lindent}{\hspace{.4cm} \Longrightarrow \hspace{.4cm}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\rank}{rank}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\setenumerate{itemsep=-1pt, label=\textbf{(\alph*)}}

\begin{document}


\maketitle
\onehalfspace
\input{disclaimer}



\section{Stochastic Growth}
Randomness! There are, of course, any number of ways we could introduce randomness into the model. Let's keep it simple for now by considering only uncertainty in technology. In particular, let $\{z_t\}$ be a sequence of i.i.d. random variables. In period $t$, the supply of goods available per worker is $y_t = z_t f(k_t)$. The weather might be nice one period and thus there will be a higher supply of goods; the next period the weather might be bad and there will be a lower supply of goods. You get the idea, and we call these \textbf{shocks}. Thus, the economy is subject to the constraint
	\[c_t + i_t + (1 - \delta)k_t = c_t + k_{t+1}  \leq z_t f(k_t).	\]
As in the deterministic case, we can assume that this actually holds as an equality when considering the forthcoming maximization problem (because wasting resources is inconsistent with utility maximization when the commodity is a ``good'').

As far as utility goes, we will assume that households rank stochastic consumption sequences according to expected utility, which is additively separable like before. In math,
	\[E[ u(c_0, c_1, ...)] = E_0 \left[ \sum_{t=0}^{\infty} \beta^t U(c_t) \right].	\]
The expectation operator $E_0$ indicates that the expected value is taken with respect to the probability distribution of the random variables $c_t$, over all $t$, based on information available in period $t=0$. 

A benevolent social planner wants to maximize expected utility subject to the constraints. At the beginning of period $t$, the current value $z_t$ of the exogenous shock is realized. The planner then knows $k_t$ and $z_t$ and thus $z_t f(k_t)$ before choosing consumption and investment. The pair $(k_t, z_t)$ is called the \textbf{state} of the economy in period $t$. 

Thus, starting at period zero, the expected utility maximization problem is
\begin{align}
	\max_{\{c_t, k_{t+1} \}_{t=0}^{\infty}} E_0\left[ \sum_{t=0}^{\infty} \beta^t U(c_t) \right] \;\; \text{ s.t. } \;\; c_t + k_{t+1} = z_t f(k_t).
\end{align}
We can once again think of the planner in period 0 as choosing an infinite sequence describing all future (and current) consumption and capital stocks. But in this stochastic setting, a sequence of \textbf{contingency plans} are chosen, one for each period. To be more specific, $c_t$ and $k_{t+1}$ are contingent upon the realization of the shocks $z_1, ..., z_t$. Beyond the initial period, the planner is aware of the sequence of realizations when the decision is being \emph{executed}, but the realizations are not known in period zero when the decision is being \emph{made}.

This sounds like it could get messy. In period 0, the social planner knows what $z_0$ is and thus what $zf(k_0)$ is. But for period 1, the planner chooses among a set of functions with an argument of $z_1$, because the planner doesn't know what $z_1$ is yet. In period 2, the planner chooses among a set of functions with an argument of $(z_1, z_2)$. In period 24598, the planner chooses among a set of functions with an argument of $(z_1, ..., z_{24598})$. Yeah, that's going to be one hell of a set of choices. But ultimately, the available capital in period 24598 is going to depend on what happened in every previous period, so this is necessary information as far determining a feasible contingency plan. 



\section{Simple Example: Two Outcomes, Two Periods}
Suppose there are only two outcomes: a good shock $a_1$ with probability $\pi_1$ and a bad shock $a_2$ with probability $\pi_2 = 1 - \pi_1$. Furthermore, we live in a world of only two periods. 

While there is ``randomness'' about the initial state with respect to $z_0$, the decision maker is aware of the realization of $z_0$ and thus can calculate the utility directly as $U(c_0)$. (So in essence, we can treat period 0 as though there is no randomness.) In period 1, however, consumption is contingent upon the unknown shock $z_1$, so we need to think in terms of expected utility. That is, the (undiscounted) expected utility of period 1 consumption is
	\[E_0[ U(c_1)]=\pi_1 U\big(c_1(a_1)\big) + \pi_2 U \big(c_1 (a_2)\big).	\]
Thus, total expected utility is
\begin{align*}
	E_0[ U(c_0, c_1)] &=U(c_0) + \beta \left [ \pi_1 U\big(c_1(a_1)\big) + \pi_2 U \big(c_1 (a_2)\big)	\right]\\
	& = U(c_0) + \beta E_0[ u(c_1)].
\end{align*}

Extending this to three periods is somewhat unpleasant because we need four contingency plans: one for each possible realization of the shocks, $(a_1, a_1)$, $(a_1, a_2)$, $(a_2, a_1)$, and $(a_2, a_2)$.  One approach is to try a nested sequence. There is a $\pi_1$ probability of getting $a_1$ in period 1, which leads to  expected utility of 
\begin{align*}	
	\beta \pi_1 \big[ U_1\big( c(a_1) \big)  + \beta \pi_1 U_2\big( c_2(a_1, a_1) \big ) + \beta \pi_2 U_2\big( c_2(a_1, a_2) \big )\big].
\end{align*}
Similarly, there is a $\pi_2$ probability of getting $a_2$ in period 1, giving expected utility
	\[\beta \pi_2 \big[ U_1\big( c(a_2) \big)  + \beta \pi_1 U_2\big( c_2(a_2, a_1) \big ) + \beta \pi_2 U_2\big( c_2(a_2, a_2) \big) \big].	\]
The sum of these exhausts what could happen in periods 1 and 2, provided we only have information from period 0. In other words, 
\begin{align*}
	\beta E_0[U(c_1, c_2)] = 	&\beta \pi_1 \big[ U_1\big( c(a_1) \big)  + \beta \pi_1 U_2\big( c_2(a_1, a_1) \big ) + \beta \pi_2 U_2\big( c_2(a_1, a_2) \big )\big]\\
	 + &\beta \pi_2 \big[ U_1\big( c(a_2) \big)  + \beta \pi_1 U_2\big( c_2(a_2, a_1) \big )
	 + \beta \pi_2 U_2\big( c_2(a_2, a_2) \big) \big].
\end{align*}
So for the total expected utility, we could write this as
\begin{align}
	E_0[ U(c_0, c_1,c_2)]  &= U_0(c_0)	 + \beta \sum_{i=1}^2 \pi_i \left[ U_1\big( c(a_i) \big) +  \beta \sum_{j=1}^2 \pi_j U_2\big( c_2(a_i, a_j) \big) \right] \\
	&= U_0(c_0) + \beta E_0[U(c_1, c_2)] . \label{simplerecursion}
\end{align}



\section{Recursion} 

It's worth noticing that the form inside the brackets of equation (\ref{simplerecursion}) mirrors the form of the entire equation -- there is recursion here that we can try to exploit. Let's get on that.

First, let's drop the number subscripts and adopt the same convention as in the previous notes. Second, since $c + k' = zf(k)$, we can write $c = zf(k) - k'$ and substitute where appropriate. Third, let's suppose the existence of the value function $v(k, z)$ that gives the maximal utility, given the state $(k,z)$ at the time the decision is made. Then we can write, given some arbitrary beginning period,
	\[v(k,z) = \max_{0 \leq k' \leq zf(k)}	\left \{ U(zf(k) - k') + \beta E[v(k', z')]\right \},\]	
bearing in mind that the expectation is taken knowing only beginning period information, and also noting that the expectation is summed (or integrated) over probabilities of \emph{future} states, i.e. $z'$ and beyond.

When we are in a stochastic environment, the policy function
	\[k_{t+1} = g(k_t, z_t)\]
is called a (first order) \textbf{stochastic difference equation}. The random variables $\{k_t\}$ generated by these equations are a (first order) \textbf{Markov process}.  Suppose we do have a policy function $k'=g(k,z)$ that describes how the social planner should choose next-period capital in optimum. Since $g(k,z)$ optimizes, its inclusion makes the $\max$ operator redundant and thus we can write
\begin{align}
	v(k,z) =	\left \{ U\big(zf(k) - g(k,z)\big) + \beta E\left[v\big(g(k,z), z'\big) \right]\right \}\\
	\text{s.t. } \;\; 0 \leq g(k,z) \leq zf(k).
\end{align}
The first order condition is analogous to that found in the deterministic case. Since $g(k,z)$ maximizes, the derivative of $v(k,z)$ with respect to $g(k,z)$ equals zero, which results in
\begin{equation}
	U'_{g(k,z)} \big( zf(k) - g(k,z) \big)	= \beta E\left[ v'_{g(k,z)}\big(g(k,z),z' \big) \right].	\label{stochasticfoc}
	\end{equation}
The envelope condition is found by taking the derivative of $v(k,z)$ with respect to $k$, which gives 
\begin{align*}
	v'_k(k, z) = 	&U'_{f(k)}\big(zf(k) - g(k,z) \big)zf'_k(k) -U'_{g(k,z)}\big(zf(k) - g(k,z) \big)g'_k(k,z)\\
	 + &E\left[v'_k\big(g(k,z), z'\big)g'_k(k,z) \right].  
\end{align*}
The tricky thing here is to recognize that the expectation is taken over probabilities of future states, but not the current state (which is already known), meaning that $g'_k(k,z)$ is independent of the sum (or integral). So we can take it out of the expectation, and at that point the two latter terms of the sum cancel out via equation (\ref{stochasticfoc}). Thus, we have the envelope condition of
\begin{equation}
	v'_k(k, z) = 	U'_{f(k)}\big(zf(k) - g(k,z) \big)zf'_k(k). \label{stochasticenvelope}
\end{equation}



\section{Cobb-Douglas Redux}


In the deterministic case, we conjectured that the policy function was $g(k)=\alpha \beta k^{\alpha}$. In the stochastic setup, let's conjecture that $g(k)= \alpha \beta zk^{\alpha}$. 


\subsection{Value Function}

We want to derive the value function explicitly in terms of $k_0$ and $z_0$. In this case, the unsolved value function is 
	\[ 
		v(k_0, z_0) = E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln( z_tk_t^{\alpha} - \alpha \beta z_t k_t^{\alpha}) \right]	.
	\]
We intend on solving this, and to that end we can rewrite the value function as 	
\begin{align*}
	v(k_0, z_0) &= E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln( z_tk_t^{\alpha} - \alpha \beta z_t k_t^{\alpha}) \right]	 \\
		&=\ln( 1 - \alpha \beta )E_0 \left[ \sum_{t=0}^{\infty} \beta^t \right]+ E_0 \left[  \sum_{t=0}^{\infty} \beta^t\ln(z_t) \right] + \alpha E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln(k_t) \right]	 \\
	&=\frac{\ln( 1 - \alpha \beta )}{1 - \beta}+ E_0 \left[  \sum_{t=0}^{\infty} \beta^t\ln(z_t) \right] + \alpha E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln(k_t) \right].
\end{align*}

This is going to be a lengthy process, so prepare yourself. The first term needs no work, so let's analyze the middle term. In particular, we are taking the expected value of a sum, and since the expected value operator is linear, we can take the sum of the expected values,
\begin{align*}
	E_0 \left[  \sum_{t=0}^{\infty} \beta^t\ln(z_t) \right]  & = E_0[\ln(z_0)] + E_0[\beta \ln(z_1)] + E_0[\beta^2 \ln(z_2)]  + \hdots	\\
	& = E_0[\ln(z_0)] + \beta E_0[\ln(z_1)] + \beta^2E_0[ \ln(z_2)]  + \hdots
\end{align*}
Since $z_t$ is some random variable, it follows that $\ln(z_t)$ is a random variable and thus has some expected value. So let $E_0[\ln(z_t)] = \mu$. Keeping in mind that $z_0$ isn't really random, we have
\begin{align*}
	E_0 \left[  \sum_{t=0}^{\infty} \beta^t\ln(z_t) \right]  = \ln(z_0) +  \beta \mu + \beta^2 +  \hdots = \ln(z_0) + \frac{\beta \mu}{1 - \beta}.
\end{align*}
So we have been able to evaluate the first two terms quite nicely.

Now let's try to get a hold of the third term, which is the most difficult of the three. Writing it out explicitly, it is
\begin{align*}
 \alpha E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln(k_t) \right]
 	&= \alpha E_0[\ln(k_0)] + \alpha \beta E_0[\ln(k_1)] + \alpha \beta^2  E_0[\ln(k_2)] +  \hdots \\
 		&= \alpha \ln(k_0) +  \alpha \beta E_0[\ln(k_1)] + \alpha \beta^2  E_0[\ln(k_2)] +  \hdots
 \end{align*}
We are going to use the conjectured policy function to write everything in terms of $k_0$. For instance, $k_1= \alpha \beta z_0k_0^{\alpha}$, so we can write the second term as 
\begin{align*}
	\alpha \beta E_0[\ln(k_1)] &=  \alpha \beta E_0[\ln\big(\alpha \beta z_0k_0^{\alpha}\big)]\\
		&= \alpha \beta E_0[\ln(\alpha \beta) + \ln(z_0) + \alpha \ln(k_0)]\\
		&= \alpha \beta \ln(\alpha \beta) + \alpha \beta \ln(z_0) + \alpha^2 \beta \ln(k_0).
\end{align*}
Now use the policy function to write $k_2$ in terms of $k_0$, that is, 
\[k_2= \alpha \beta z_1k_1^{\alpha} = \alpha \beta z_1(\alpha \beta z_0k_0^{\alpha})^{\alpha}.	\]	
So take the third term in the sum and substitute:
\begin{align*}
	\alpha \beta^2  E_0[\ln(k_2)] &= 	\alpha \beta^2  E_0\big[\ln( \alpha \beta z_1[\alpha \beta z_0k_0^{\alpha}]^{\alpha})\big] \\
	&= 	\alpha \beta^2  E_0\big[\ln( \alpha \beta)+ \ln(z_1) + \alpha\ln(\alpha \beta z_0k_0^{\alpha})\big] \\
	&= 	\alpha \beta^2  E_0\big[\ln( \alpha \beta)\big]+ \alpha \beta^2E_0\big[\ln(z_1)\big] + \alpha^2 \beta^2E_0\big[\ln(\alpha \beta z_0k_0^{\alpha})\big] \\
	&= 	\alpha \beta^2 \ln( \alpha \beta)+ \alpha \beta^2\mu + \alpha^2 \beta^2E_0\big[\ln(\alpha \beta) + \ln( z_0) + \alpha \ln(k_0)\big] \\
	&= 	\alpha \beta^2 \ln( \alpha \beta)+ \alpha \beta^2\mu + \alpha^2 \beta^2\ln(\alpha \beta) + \alpha^2 \beta^2\ln( z_0) + \alpha^3 \beta^2 \ln(k_0).
\end{align*}
Okay, so let's see what we have so far. Summing the three expanded terms, we have
\begin{align*}
	&\hspace{.5cm} \alpha \ln(k_0) \\
	&+\alpha^2 \beta \ln(k_0) &+& \alpha \beta \ln(z_0)  &+& \alpha \beta \ln(\alpha \beta)   \\
	&+\alpha^3 \beta^2 \ln(k_0) &+& \alpha^2 \beta^2\ln( z_0) &+& \alpha \beta^2 \ln( \alpha \beta) &+&  \alpha^2 \beta^2\ln(\alpha \beta) &+& \alpha \beta^2\mu \\
	&+ \hdots &+& \hdots &+& \hdots &+& \hdots &+&\hdots
\end{align*}
The first and second columns give respective sums of
	\[ \frac{\alpha \ln(k_0)}{1 - \alpha \beta } , \indent \frac{\alpha \beta \ln(z_0)}{1 - \alpha \beta }.	\]
The third and fourth columns, appealing to the previous notes, respectively give
	\[ \frac{\alpha \beta \ln(\alpha \beta)}{1 - \beta}, \indent  \frac{\alpha^2 \beta^2 \ln(\alpha \beta)}{1 - \beta}, \]
which in turn form their own sequence that evaluates to
	\[ \frac{\alpha \beta}{1 - \beta}\frac{\ln(\alpha \beta)}{1 - \alpha \beta}.	\]

The only thing unaccounted for is that $\alpha \beta^2 \mu$ term. We only have one line to go on, but it might take on the same form as the middle column. To quickly confirm, write $k_3= \alpha \beta z_2k_2^{\alpha}$, and therefore
	\begin{align*}
		 \alpha \beta^3  E_0[\ln(k_3)] &=		 \alpha \beta^3  E_0[\ln(\alpha \beta z_2k_2^{\alpha})]\\
		 &= \alpha \beta^3 E_0[\ln(z_2) + \ln(\alpha \beta k_2^{\alpha})]\\
		 &= \alpha \beta^3 E_0[\ln(z_2)]+  \alpha \beta^3 E_0[\ln(\alpha \beta k_2^{\alpha})]\\
		 &= \alpha \beta^3 \mu + \alpha \beta^3 E_0[\ln(\alpha \beta k_2^{\alpha})].
	\end{align*}
We're not quite done yet, however. Notice that we'll eventually substitute in for $k_2$ something that is in terms of $k_0$. In fact, we'd use $k_2= \alpha \beta z_1(\alpha \beta z_0k_0^{\alpha})^{\alpha}$, which will introduce another $\mu$ term, in particular (just from eyeballing it), $\alpha^2 \beta^3 \mu$. This amounts to anther column. More explicitly,
\begin{align*}
	&\hspace{.5cm} \alpha \ln(k_0) \\
	&+\alpha^2 \beta \ln(k_0) &+& \alpha \beta \ln(z_0)  &+& \alpha \beta \ln(\alpha \beta)   \\
	&+\alpha^3 \beta^2 \ln(k_0) &+& \alpha^2 \beta^2\ln( z_0) &+& \alpha \beta^2 \ln( \alpha \beta) &+&  \alpha^2 \beta^2\ln(\alpha \beta) &+& \alpha \beta^2\mu \\
	&+ \hdots &+& \hdots &+& \hdots &+& \hdots &+&\alpha \beta^3 \mu &+& \alpha^2 \beta^3 \mu  &+& \hdots	 \\
	&+ \hdots &+& \hdots &+& \hdots &+& \hdots &+&\hdots &+& \hdots &+& \hdots
\end{align*}
Yeah, this is a lot, I know. The fifth and sixths columns will sum respectively to
	\[ \frac{\alpha \beta^2 \mu}{1 - \beta}, \indent \frac{\alpha^2 \beta^3 \mu}{1 - \beta},	\indent \hdots \]
which themselves form a series that can be evaluated to
	\[ \frac{\alpha \beta^2 \mu}{1 - \beta}\frac{1}{1 - \alpha \beta}.	\]
So, all in all, we have
	\[\alpha E_0 \left[ \sum_{t=0}^{\infty} \beta^t \ln(k_t) \right] =  \frac{\alpha \ln(k_0)}{1 - \alpha \beta} + \frac{\alpha \beta \ln(z_0)}{1 - \alpha \beta} + \frac{\alpha \beta}{1 - \beta} \frac{\ln(\alpha \beta)}{1 - \alpha \beta} + \frac{\alpha \beta^2 \mu}{1 - \beta}\frac{1}{1 - \alpha \beta}. \]
Great, so that takes care of the last sum.

Putting them all together and simplifying a little, we get the rather monstrous
\begin{equation}
	v(k_0,z_0) =  \frac{\ln(1 - \alpha \beta) + \beta \mu}{1 - \beta} + \frac{\alpha \beta \ln(\alpha \beta)+\alpha \beta^2 \mu}{(1 - \beta)(1 - \alpha \beta)} + \frac{\alpha \ln(k_0)}{1 - \alpha \beta} + \frac{\ln(z_0)}{1 - \alpha \beta}, \label{cdstochasticvalue}
\end{equation}
which you might notice is of the form $A + B\ln(k_0) + C\ln(z_0)$.

So yeah, this has been a rather long and tedious exercise. It's worth considering other approaches that could be less of a pain in the ass. For instance, we might try to come up with a way of expressing $\ln(k_t)$ for an arbitrary $t$ in terms of $k_0$. And this \emph{is} possible, and probably preferable because you don't have to make the same extrapolations I had to make. It is still a pain in the ass, however. The more practical lesson, I think, is that \emph{computers are your friend}, since there is little meaningful that can be learned by doing this manually. (Unless you think PhD-level economics should be testing primarily your algebra skills. Some people evidently seem to think that it should. Hey, don't ask me.)


\subsection{Verifying Conditions}

We should be thorough and show that equation (\ref{cdstochasticvalue}) satisfies the first order and envelope conditions. To do so, let's write the value function in terms of the Bellman equation, specifically,
	\[v(k,z)=\ln(z k^{\alpha} - \alpha \beta zk^{\alpha})  + \beta[A + B\ln(\alpha \beta zk^{\alpha}) + C\ln(z')]. \]
The first order condition says to take the derivative with respect to $g(k,z)=\alpha \beta zk^{\alpha}$, which should be equal to zero because this is supposed to be a maximum. Doing so, we get
	\[	\frac{1}{z k^{\alpha} - \alpha \beta zk^{\alpha}} = \frac{\beta B}{\alpha \beta zk^{\alpha}} \Lindent \frac{\alpha}{ 1- \alpha \beta } = B. \]
Great, so the first order condition checks out.

Testing the envelope condition, we want to take the derivative with respect to $k$. Doing so results in
\begin{align*}
	v'_k(k,z) &= \frac{1}{z k^{\alpha} - \alpha \beta zk^{\alpha}}(\alpha zk^{\alpha - 1} - \alpha^2 \beta zk^{\alpha - 1})	+ \frac{\beta B}{\alpha \beta zk^{\alpha}} \alpha^2 \beta zk^{\alpha - 1} \\
	& = \frac{\alpha}{ k(1 - \alpha \beta)}(1  - \alpha \beta )	+ \frac{ B}{k} \alpha \beta\\
	& = \frac{\alpha}{ k(1 - \alpha \beta)}(1  - \alpha \beta )	+  \frac{\alpha}{ 1- \alpha \beta }\frac{ \alpha \beta}{k}\\
	& = \frac{\alpha}{ k(1 - \alpha \beta)}(1  - \alpha \beta )	+  \frac{\alpha}{ 1- \alpha \beta }\frac{ \alpha \beta}{k}\\
	&= \frac{\alpha}{(1 - \alpha \beta)k}.
\end{align*}
This must equal equation (\ref{stochasticenvelope}), which gives
\begin{align*}
	U'_{f(k)}\big(zf(k) - g(k,z) \big)zf'_k(k) &= \frac{1}{zk^{\alpha} -  \alpha \beta zk^{\alpha}}\alpha zk^{\alpha - 1}\\
	 &= \frac{\alpha}{(1 -  \alpha \beta) k}.
\end{align*}
Success!

	
\end{document}
 
 	