\title{Econometrics -- Maximum Likelihood Estimation}
\author{William M Volckmann II}
\documentclass[12pt]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{xifthen}
\usepackage{titlesec}
\usepackage[normalem]{ulem}
\usepackage[final]{pdfpages}
%\usepackage[top=1.25in, left=1.25in, right=1.25in]{geometry}

\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Chi}{\mathcal{X}}
\newcommand{\grad}{\nabla}
\newcommand{\BH}{\hat{\beta}}
\newcommand{\bh}{\hat{\beta}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\crit}{c_{\alpha}}
\newcommand{\given}{\; | \;}
\newcommand{\xbar}{\overline{X}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\asim}{\overset{a}{\sim}}
\newcommand{\norm}[1]{\left\Vert{#1}\right\Vert}
\newcommand{\abs}[1]{\left\vert{#1}\right\vert}
\newcommand{\normal}[2]{\mathcal{N} \left({#1}, {#2} \right)}
\newcommand{\cprob}{\overset{P}{\rightarrow}}
\newcommand{\cdist}{\overset{D}{\rightarrow}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\hat}[1]{\widehat{#1}}
\newcommand{\Lindent}{\hspace{.4cm} \Longrightarrow \hspace{.4cm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\Var}{Var}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\setenumerate{itemsep=-1pt, label=\textbf{(\alph*)}}

\begin{document}


\maketitle
\onehalfspace


\noindent \emph{This set of notes is... not great. It's based on material that was presented in lecture that itself was... not great. So there are some things missing, it's lacking in exposition, and generally is just... not great. }


\section{MLE Redux}

We already know a little bit about maximum likelihood estimation (mle). We've seen the \textbf{likelihood function},
	\[L(\theta;x) = \prod_{i=1}^n f(x_i;\theta),	\]
where we treat $L$ as a function of $\theta$, hence often writing $L(\theta)$. We've used the \textbf{log likelihood function},
	\[l(\theta) = \ln\big(L(\theta)\big) = \sumn \ln\big(f(x_i;\theta) \big),	\]
which is often easier to work with than the likelihood function itself. The maximizer of $L(\theta)$, which is also the maximizer of $l(\theta)$, is our point estimator for $\theta$, which we denote $\widehat{\theta}$ and call it the \textbf{maximum likelihood estimator} of $\theta$. In particular, the mle solves the \textbf{estimating equation}
	\[\frac{\partial l(\theta)}{\partial \theta} =0.	\]


We will often rely on the following \textbf{regularity conditions}.
\begin{enumerate}[label=\textbf{(R\arabic*)}]
	\item The pdfs are distinct for each $\theta$. That is, if $\theta \neq \theta'$, then $f(x_i;\theta) \neq f(x_i;\theta')$. 
	\item The pdfs have common support for all $\theta$.
	\item The \textbf{true value} of $\theta$, denoted $\theta_0$, is an interior point of the sample space. 
\end{enumerate}
 We will also be assuming that the true data generating process is $f(x; \theta_0)$, that is, assuming that the density is correctly specified. 
 
 \begin{theorem}
 	Suppose that $X_1, \hdots, X_n$ satisfy the regularity conditions R1-R3, where $\theta_0$ is the true parameter. Furthermore, suppose that $f(x;\theta)$ is differentiable with respect to $\theta$. Then the likelihood equations,
 		\[\frac{\partial L(\theta)}{\partial \theta}=0 \quad \text{and} \quad \frac{\partial l(\theta)}{\partial \theta}=0,	\]
have a solution $\thetahat_n$ such that $\thetahat_n \cprob \theta_0$. 
 \end{theorem}
 
 \begin{theorem}
 	Suppose that $X_1, \hdots, X_n$ satisfy the regularity conditions R1-R3, where $\theta_0$ is the true parameter. Furthermore, suppose that $f(x;\theta)$ is differentiable with respect to $\theta$. Suppose the likelihiid equation has the unique solution $\thetahat_n$. Then $\thetahat_n$ is a consistent estimator of $\theta_0$. 
 \end{theorem}
 
\begin{theorem}
	Let $X_1, \hdots, X_n$ be iid with the pdf $f(x;\theta)$. For a specified function $g$, let $\eta =g(\theta)$ be a parameter of interest. Suppose $\hat{\theta}$ is the mle of $\theta$. Then $\thetahat$ is the mle of $\eta = g(\theta)$. 
\end{theorem}




\section{Newton's Method of Iteration}

Sometimes there might not actually be an analytical solution to the mle. In such a case, we can use numerical analysis by way of \textbf{Newton's Method of Iteration} to approximate the solution. We make an initial guess at the solution, $\widehat{\theta}_0$. Take the second-order Taylor expansion of $l(\theta)$ around $\hat{\theta}_0$,
	\[
		l(\theta) \approx l(\hat{\theta}_0) + l'(\hat{\theta}_0)(\theta - \hat{\theta}_0) + \frac{1}{2} l''(\hat{\theta}_0)(\theta - \hat{\theta}_0)^2.	
	\]	
Then take the derivative with respect to $\theta$, set it equal to zero, and solve for $\theta$ to get the subsequent iteration, 
	\[l'(\hat{\theta}_0) +  l''(\hat{\theta}_0)(\theta - \hat{\theta}_0):=0	\Lindent	\hat{\theta}_1 = \hat{\theta}_0 - \frac{l'(\hat{\theta}_0)}{l''(\hat{\theta}_0)}.\]
Proceed with the same chain of logic but with the new guess $\hat{\theta}_1$. Continue with the series of iterative guesses
	\[\hat{\theta}_1 = \hat{\theta}_0 - \frac{l'(\hat{\theta}_0)}{l''(\hat{\theta}_0)} \Lindent  \hat{\theta}_2 = \hat{\theta}_1 - \frac{l'(\hat{\theta}_1)}{l''(\hat{\theta}_1)} \Lindent \hdots	\]
and so on. In general, the $s$th iteration will be
	\[	\hat{\theta}_{s} = \hat{\theta}_{s-1} - \frac{l'(\hat{\theta}_{s-1})}{l''(\hat{\theta}_{s-1})}.		\]
Iteration continues until the change in each iteration and the change in $l(\hat{\theta}_s)$ is deemed sufficiently small. 




\section{Efficiency}

 We will now introduce two additional regularity conditions.
\begin{enumerate}[label=\textbf{(R\arabic*)}]
\setcounter{enumi}{3}
	\item The pdf $f(x;\theta)$ is twice differentiable as a function of $\theta$.
	\item The integral $\int f(x;\theta)\; dx$ can be differentiated twice under the integral sign as a function of $\theta$. 
\end{enumerate}
 



\subsection{Fisher Information}

Since $f(x;\theta)$ is a pdf, it follows that 
	\[	\int_{-\infty}^{\infty} f(x;\theta) \; dx = 1.	\]
Supposing the integrals and derivatives are interchangeable, we can take the derivative with respect to $\theta$ to get
	\[	\int_{-\infty}^{\infty} \frac{\partial f(x;\theta)}{\partial \theta} \; dx = 0,	\]
which, by introducing $f(x;\theta)/f(x;\theta)$, can be written 
	\[	\int_{-\infty}^{\infty} \frac{\partial f(x;\theta)/ \partial \theta}{f(x;\theta)}f(x;\theta) \; dx = 0.	\]
Note that for any function $g(y)$, we have
	\[\frac{\partial \ln\big( g(y)\big)}{\partial y} = \frac{1}{g(y)}\frac{\partial g(y)}{\partial y} \quad \text{and} \quad \frac{\partial \ln\big( g(y)\big)}{\partial y} g(y)=\frac{\partial g(y)}{\partial y}.	\]
Keeping in mind that we are treating $f$ as a function of $\theta$, we thus can write
\begin{equation}
	\int_{-\infty}^{\infty} \frac{\partial \ln\big( f(x;\theta)\big)}{\partial \theta }f(x;\theta) \; dx = 0.	\label{beforeexp1}
\end{equation}
Differentiate one more time, applying the product rule, to get 
\begin{align}
	&\hspace{.45cm} \int_{-\infty}^{\infty} \frac{\partial^2 \ln\big( f(x;\theta)\big)}{\partial \theta^2 }f(x;\theta) + \frac{\partial \ln\big( f(x;\theta)\big)}{\partial \theta }\frac{\partial f(x;\theta)}{ \partial \theta} \; dx \nonumber \\
	& = \int_{-\infty}^{\infty} \frac{\partial^2 \ln\big( f(x;\theta)\big)}{\partial \theta^2 }f(x;\theta) \;dx + \int_{-\infty}^{\infty}  \frac{\partial \ln\big( f(x;\theta)\big)}{\partial \theta } \frac{\partial \ln\big( f(x;\theta) \big)}{\partial \theta} f(x;\theta) \; dx \nonumber \\
	& = \int_{-\infty}^{\infty} \frac{\partial^2 \ln\big( f(x;\theta)\big)}{\partial \theta^2 }f(x;\theta) \;dx + \int_{-\infty}^{\infty}  \left [\frac{\partial \ln\big( f(x;\theta)\big)}{\partial \theta }\right]^2 f(x;\theta) \; dx  \label{beforeexpect} \\
	&= 0. 	\nonumber 
\end{align}

The second term in equation (\ref{beforeexpect}) can be written as an expectation. We refer to it as \textbf{Fisher information} and denote it with $I(\theta)$:
	\[ I(\theta) = E\left[    \left (\frac{\partial \ln\big( f(X;\theta)\big)}{\partial \theta }\right)^2 \right].	\]
Alternatively, from equation (\ref{beforeexpect}) we can write
	\[ I(\theta) = -E\left[  \frac{\partial^2 \ln\big( f(X;\theta)\big)}{\partial \theta^2 } \right]. 	\]
Usually the second form involving the second derivative is easier to compute. 

Notice that equation (\ref{beforeexp1}) can also be written as an expectation, in particular, as
	\[	
		E \left[  \frac{\partial \ln\big( f(X;\theta)\big)}{\partial \theta }\right] =0.
	\]
Let $Y =   \partial \ln\big( f(X;\theta)\big)/\partial \theta$. Then we can write
	\[	\Var(Y) = E[Y^2] - E[Y]^2  = I(\theta).\] 
Thus, Fisher information is the variance of the random variable $ \partial \ln\big( f(X;\theta)\big)/\partial \theta$. The function $\partial \ln\big( f(x;\theta)\big)/\partial \theta$ is called the \textbf{score function}. Recall that it determines the estimating equations for the mle -- recall that the mle $\thetahat$ solves
	\[\sumn  \frac{\partial \ln\big( f(x_i;\theta)\big)}{\partial \theta}=0.	\]




\subsection{Rao-Cramer Lower Bound} 

When we have only one sample, say $X_1$, Fisher information is the variance of the random variable $\partial \ln\big( f(X_1;\theta)\big)/\partial \theta$. For sample size $n$, the likelihood $L(\theta)$ is the pdf of the random sample, and the random variable whose variance is the information in the sample is given by
	\[	\frac{\partial \ln \big( L(\theta, \vec{X})\big)}{\partial \theta} = \sumn \frac{\partial \ln \big( f(X_i;\theta) \big)}{\partial \theta} .\]
Each summand is iid with common variance $I(\theta)$. Recall that the variance of the sum of independent random variables is the sum of the variance of those variables. Thus, we simply have
	\[ \Var\left( \frac{\partial \ln \big( f(\theta,\vec{X})\big)}{\partial \theta} \right) = nI(\theta). 	\]

\begin{theorem}
	Let $X_1, \hdots, X_n$ be iid with common pdf $f(x;\theta)$. Assume that the regularity conditions R1-R5 hold. Let $Y=u(X_1, \hdots, X_n)$ be a statistic with mean $E[Y]=k(\theta)$. Then
		\[	\Var(Y) \geq \frac{[k'(\theta)]^2}{nI(\theta)}.\]
\end{theorem}

\begin{corollary}
	Suppose $Y=u(X_1, \hdots, X_n)$ is an unbiased estimator of $\theta$, so that $k(\theta)=\theta$. Then the Rao-Cramer inequality becomes
		\[	\Var(Y) \geq \frac{1}{nI(\theta)}.	\]
\end{corollary}

\begin{definition}
	Let $Y$ be an unbiased estimator of a parameter $\theta$ in the case of point estimation. The statistic $Y$ is called an \textbf{efficient estimator} of $\theta$ if and only if the variance of $Y$ attains the Rao-Cramer lower bound. 
\end{definition}

\begin{definition}
	In cases in which we can differentiate with respect to a parameter under an integral or summation symbol, the ratio of the Rao-Cramer lower bound to the actual variance of an any unbiased estimator of a parameter is called the \textbf{efficiency} of that estimator.
\end{definition}




\section{Asymptotics}

Let's impose one more regularity condition.
\begin{enumerate}[label=\textbf{(R\arabic*)}]
	\setcounter{enumi}{5}
	\item The pdf $f(x;\theta)$ is three times differentiable as a function of $\theta$. Further, for all $\theta$, there exists a constant $c$ and a function $M(x)$ such that
		\[	\abs{ \frac{\partial^3}{\partial \theta^3} \ln\big( f(x;\theta)\big)} \leq M(x),\]	
	which $E_{\theta_0}[M(X)] \leq \infty$ for all $\theta_0 - c < \theta < \theta_0 + c$ and all $x$ in the support of $X$. 
\end{enumerate}
Yeah, that's a hell of a condition. But the proceeding theorem, which R6 justifies, is very nice. 

\begin{theorem}
	Assume $X_1, \hdots, X_n$ are iid with pdf $f(x;\theta_0)$ such that the regularity conditions R1-R6 are satisfied. Suppose further than the Fisher information satisfies $0 < I(\theta_0) < \infty$. Then any consistent sequence of solutions of the mle equations satisfies
		\[	\sqrt{n}(\thetahat - \theta_0) \cdist \normal{0}{\frac{1}{I(\theta_0)}}.	\]
\end{theorem}
As in the previous notes, we can write the result of the previous theorem with regard to asymptotic distribution, i.e. for large (but not limiting to infinity) $n$, as
		\[	\thetahat  \asim \normal{\theta_0}{\frac{1}{nI(\theta_0)}}.	\]
This theorem also implies that $I(\thetahat) \cprob I(\theta_0)$. As a consequence, the interval
	\[ \left( \thetahat_n - z_{\alpha/2} \frac{1}{\sqrt{nI(\thetahat_n)}},  \thetahat_n + z_{\alpha/2} \frac{1}{\sqrt{nI(\thetahat_n)}} \right )	\]
is an approximate $(1 - \alpha)100\%$ confidence interval for $\theta$. 


\begin{corollary}
	Assume $X_1, \hdots, X_n$ are iid with pdf $f(x;\theta_0)$ such that the regularity conditions R1-R6 are satisfied. Suppose further than the Fisher information satisfies $0 < I(\theta_0) < \infty$.  Finally, suppose $g(x)$ is a continuous function of $x$ which is differentiable at $\theta_0$ such that $g'(\theta_0) \neq 0$. Then
	\[	\sqrt{n}\left[ g(\thetahat_n) - g(\theta_0) \right] \cdist \normal{0}{ \frac{g'(\theta_0)^2}{I(\theta_0)} }.	\]	
\end{corollary}
This should remind you of the $\Delta$-method, and indeed, it is an application of the $\Delta$-method to the previous theorem. 




\section{Failure of Assumptions}

Sometimes we might specify the wrong density function $f(x;\theta)$ and do the typical MLE routine. The the MLE is called a \textbf{quasi-MLE} or \textbf{pseudo-MLE}. In general, the quasi-MLE is \emph{inconsistent} for $\theta_0$, and the information equality does \emph{not} hold. In other words, we should not use the MLE. This can't be all that surprising, can it? 

However, if the specified density $f(x;\theta)$ is in the exponential family, then the quasi-MLE remains consistent for $\theta_0$. 




\section{Maximum Likelihood Tests}

We will be considering a two-sided test of $H_0: \theta = \theta^*$ vs $H_1: \theta \neq \theta^*$ at level $\alpha$. There are three maximum likelihood tests we can do concerning whether or not to reject the null.
\begin{enumerate}
	\item  They are asymptotically equivalent under $H_0$.
	\item They are asymptotically equivalent under local alternatives, i.e. $H_1: \theta = \theta^* + c/\sqrt{n}$. 
	\item They are all $\chi^2(1)$ distributed.
\end{enumerate}
Thus, we can use whichever test is simplest given the context. 


\subsection{The Likelihood Ratio Test}
Consider the ratio of two likelihood functions. In particular, define
	\[	\Lambda = \frac{L(\theta^*)}{L(\thetahat)}.\]
If we assume regularity conditions R1-R6 all hold, then under the null hypothesis $H_0 : \theta=\theta^*$, we have
	\[-2 \ln (\Lambda) \cdist \chi^2(1).	\]
Thus, we reject $H_0$ if $-2\ln(\Lambda) \geq \chi_{\alpha}^2(1)$. 	Equivalently, reject $H_0$ if
	\[	2 \ln[L(\thetahat)-L(\theta^*)] \geq \chi_{\alpha}^2(1).	\]
We call this the \textbf{likelihood ratio test}. It is favored by statisticians due to the Neyman-Pearson lemma, which we will cover later. It relies entirely on correct specification of the density, however. 
	

\subsection{The Wald Test}
The \textbf{Wald test} relies on the statistic
	\[	\chi_W^2 = \left[\sqrt{n I(\thetahat)} ( \thetahat - \theta^*) \right] ^2.\]
We reject $H_0$ if $\chi_W^2 \geq \chi_{\alpha}^2(1)$. Note that for a one-sided test, we can use $\chi_W \cdist \normal{0}{1}$. 

The Wald test is favored by econometricians because it is easy to robustify. 



\subsection{The Score Test}
Define the statistic
	\[\chi_R^2 = \left[\frac{l'(\theta^*)}{\sqrt{nI(\theta^*)}}\right]^2.	 \]	
We reject $H_0$ in favor of $H_1$ if $\chi^2_R \geq \chi_{\alpha}^2(1)$.  This is called the \textbf{score test}. Note that for a one-sided test, we can use $\chi_R \cdist \normal{0}{1}$. 

The score test is advantageous when it is much simpler to estimate the model under $H_0$. 




\section{Optimality of Maximum Likelihood Tests}

Recall that the size of a test is the probability of rejecting $H_0$ given that  $H_0$ is actually true; and the power of a test is the probability of rejecting $H_0$ given that $H_1$ is true. A test a fixed size $\alpha$ will be unbiased if power is always greater than or equal to the size; and it will be consistent if power approaches 1 as $n$ approaches infinity. The idea is that if we have a lot of data, we should have more ability to reject false nulls. 

A \textbf{most powerful test} is a test that has equal or higher power than any other test. A most powerful test exists for testing one simple hypothesis against another, i.e. $H_0:\theta=\theta'$ vs $H_1: \theta=\theta''$. By the Neyman-Pearson lemma, this is a function of the likelihood ratio. Which is ti say, a most powerful test reject $H_0$ if
	\[	\frac{L(\theta';x)}{L(\theta'';x)} \leq k \quad \Longleftrightarrow \quad \ln\big( L(\theta';x) \big) - \ln\big( L(\theta'';x)\big) \leq \ln(k), \]
where $k$ is determined by the test size $\alpha$. 

Now consider a simple null vs a composite alternative, e.g. $H_0: \theta = \theta'$ vs $H_1: \theta > \theta'$. A \textbf{uniformly most powerful test} of size $\alpha$ is one that has the most power against every value of $\theta''$ under $H_1$. A uniformly most powerful test does not always exist, but it does for sure when there is a monotone likelihood ratio. 



\end{document}
