\title{Dynamic Programming -- Vector Spaces, Sequences, and Convergence}
\author{William M Volckmann II}
\documentclass[12pt]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{xifthen}
\usepackage{titlesec}
\usepackage[normalem]{ulem}
\usepackage[final]{pdfpages}
%\usepackage[top=1.25in, left=1.25in, right=1.25in]{geometry}

\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\Chi}{\mathcal{X}}
\newcommand{\grad}{\nabla}
\newcommand{\B}{\beta}
\newcommand{\BH}{\hat{\beta}}
\newcommand{\bh}{\hat{\beta}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\crit}{c_{\alpha}}
\newcommand{\given}{\; | \;}
\newcommand{\xbar}{\bar{X}_n}
\newcommand{\asim}{\overset{a}{\sim}}
\newcommand{\Lindent}{\hspace{.4cm} \Longrightarrow \hspace{.4cm}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\left\Vert{#1}\right\Vert}
\newcommand{\abs}[1]{\left\vert{#1}\right\vert}

\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\rank}{rank}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\setenumerate{itemsep=-1pt, label=\textbf{(\alph*)}}

\begin{document}


\maketitle
\onehalfspace
\input{disclaimer}




\section{Vector Spaces and Metric Spaces}

I'm hoping you already know what a vector space is. It's just a set of objects with some rather basic properties. And a metric space and normed space as well, so I'll be rather blunt in this section. Here, have some definitions and theorems and proofs. (Warning: these notes will be more proof-heavy than previous notes.)

\begin{definition}
A \textbf{vector space} $X$ is a set of elements (\textbf{vectors}) together with two operations, addition and scalar multiplication. For any vectors $x, y,z \in X$ and $\alpha, \beta \in \R$, the following properties must hold:
\begin{itemize}
	\itemsep0em
	\item $x + y \in X$ and $\alpha x \in X$
	\item $x + y = y + x$
	\item $(x + y) + z = x + (y  +z)$
	\item $\alpha(x+y) = \alpha x + \alpha y$
	\item $(\alpha + \beta)x = \alpha x + \beta x$
	\item $(\alpha \beta) x = \alpha (\beta x)$
	\item There exists a zero vector $0 \in X$ such that 
		\[x + 0=x, \quad 0x = 0.\]
	\item There exists an multiplicative identity $1 \in X$ such that $1x=x$. 
\end{itemize}
\end{definition}

\begin{theorem}
	The set $X$ consisting of all infinite sequences $(x_0, x_1, ..., )$ where $x_i \in \R$ forms a vector space. \label{infinitevectorspace}
\end{theorem}
\begin{proof}	
	Take $x,y,z \in \R^{\infty}$ where $x=(x_1, x_2, \hdots )$, $y=(y_1, y_2, \hdots)$, and $z=(z_1, z_2, \hdots)$, with $\alpha, \beta \in \R$. Define addition to be component-wise and scalar multiplication to multiply every element. The the addition and scalar axioms follow easily via basic arithmetic. Have the zero element be $(0, 0, \hdots)$ and the multiplicative identity be $(1, 1, \hdots)$ and the remaining axioms follow easily enough that I won't bother here. 
\end{proof}

\begin{theorem}
	The set of all continuous functions on the interval $[a,b]$ form a vector space. \label{continuousvs}
\end{theorem}
\begin{proof}
	Recall that a function is continuous if $x_n \rightarrow x$ implies that $f(x_n) \rightarrow f(x)$. Have $f, g :[a,b] \rightarrow \R$ be arbitrary continuous functions. Also take  $\alpha \in\R$. Define function addition and scalar multiplication, respectively, to be
		\[(f + g)(x) = f(x) + g(x), \quad (\alpha f)(x) = \alpha f(x).	\]
		Now take some sequence $x_n \rightarrow x$. Since $f$ and $g$ are both continuous, we have $f(x_n) \rightarrow f(x)$ and $g(x_n) \rightarrow g(x)$. From here, most of the properties are pretty easy to show so I will omit them. The closure of addition is what we're concerned about. What we have is
\begin{align*}
	\lim_{x_n \rightarrow \infty}(f + g)(x_n) &=   \lim_{x_n \rightarrow \infty}[f(x_n) + g(x_n)] \\
		&= \lim_{x_n \rightarrow \infty} f(x_n) + \lim_{x_n \rightarrow \infty}g(x_n)\\
			& = f(x) + g(x)\\
			& = f(x + g).			
\end{align*}
Thus, the addition of continuous functions is also continuous. 
\end{proof}


\begin{definition}
 A \textbf{metric space} is a set $S$ together with a metric (distance function) $d:S \times X \rightarrow \R$ such that for all $x, y, z \in S$, we have
 	\begin{itemize}
 	\itemsep0em
 	\item $d(x,y) \geq 0$ with equality if and only if $x=y$
 	\item $d(x,y)=d(y,x)$
 	\item $d(x,z) \leq d(x,y) + d(y,z)$
 	\end{itemize} 	
\end{definition}

\begin{definition}
	A \textbf{normed vector space} is a vector space $S$, together with a norm $\norm{\cdot} : S \rightarrow \R$ such that for all $x, y \in S$ and $\alpha \in \R$, we have
	\begin{itemize}
		\itemsep0em
		\item $\norm{x} \geq 0$ with equality if and only if $x = 0$
		\item $\norm{\alpha x} = \abs{\alpha} \cdot \norm{x}$
		\item $\norm{x + y} \leq \norm{x} + \norm{y}$
	\end{itemize}
\end{definition}


\begin{theorem}
The set $S$ of all bounded infinite sequences $(x_1, x_2, ...)$ such that $x_k \in \R$ with $\norm{x} = \sup_k \abs{x_k}$ forms a normed vector space. (This space is called $l_{\infty}$.) \label{boundednormedspace}
\end{theorem}
\begin{proof}
	We know from Theorem \ref{infinitevectorspace} that the set of infinite sequences is a vector space. Adding boundedness doesn't change the proofs, and bounded sums and bounded scalar products will themselves be bounded, so it will be closed. This makes $S$ a vector \textbf{subspace} of $X$. 
	
	The more elucidating goal here is to prove that $S$ has a norm.
Since all $\abs{x_k} \geq 0$, it follows that $\sup_k \abs{x_k} \geq 0$. Thus, $\norm{x} \geq 0$. Furthermore, the only way we can have $\norm{x}=0$ is if $\sup_k \abs{x_k}=0$, which can only be true if every $x_k=0$. And of course, if every $x_k=0$, then $\sup_k \abs{x_k}=0$ so that $\norm{x}=0$.  

The second property holds because
		\begin{align*}
			\norm{\alpha x} &= \sup_k \abs{\alpha x_k} \\
					&= \sup_k \abs{\alpha} \abs{x_k}\\
					&= \abs{\alpha} \sup_k \abs{x_k} \\
					&= \abs{\alpha} \norm{x}.
		\end{align*}
The third property holds because
		\begin{align*}
			\norm{x + y} &= \sup_k \abs{x_k + y_k}\\
				&\leq \sup_k (\abs{x_k} + \abs{y_k})\\
				&\leq \sup_k \abs{x_k} + \sup_k \abs{y_k}\\
				& = \norm{x} + \norm{y}. \qedhere
		\end{align*}
\end{proof}


\begin{theorem}
	The set $S$ of all continuous functions on $[a,b]$ with $\norm{x}=\sup_{a \leq t \leq b} \abs{x(t)}$ forms a normed vector space. (This called is called $C[a,b]$.)
\end{theorem}
\begin{proof}	
	We already know from Theorem \ref{continuousvs} that this will form a vector space, so what we need to establish here is the norm. The proof is very similar to the previous proof. Since $\abs{x(t)} \geq 0$ for all $t$, it follows that $\norm{x} = \sup_{a \leq t \leq b} \abs{x(t)} \geq 0$.
	
The second property holds because
	\begin{align*}
			\norm{\alpha x} &=  \sup_{a \leq t \leq b}  \abs{\alpha x(t)} \\
					&= \sup_{a \leq t \leq b} \abs{\alpha} \abs{x(t)}\\
					&= \abs{\alpha} \sup_{a \leq t \leq b} \abs{x(t)} \\
					&= \abs{\alpha} \norm{x}.
		\end{align*}
The third property holds because
		\begin{align*}
			\norm{x + y} &= \sup_{a \leq t \leq b} \abs{x(t) + y(t)}\\
				&\leq \sup_{a \leq t \leq b} (\abs{x(t)} + \abs{y(t)})\\
				&\leq \sup_{a \leq t \leq b} \abs{x(t)} + \sup_{a \leq t \leq b} \abs{y(t)}\\
				& = \norm{x} + \norm{y}. \qedhere
		\end{align*}
\end{proof} 




\section{Sequences} 

\begin{definition}
	A sequence $\{x_n\}_{n=0}^{\infty}$ in $S$ \textbf{converges} to $x \in S$ if for every $\epsilon > 0$ there exists in $N_{\epsilon}$ such that 
	\[d(x_n, x) < \epsilon \quad \text{ for all } n \geq N_{\epsilon}.	\]
\end{definition}

In other words, if we want to get arbitrarily close to $x$, we just have to go far enough out in the sequence and then we will always be at least that close. Another way of looking at this is that the distance between $x_n$ and $x$ converges to zero. We can write $x_n \rightarrow x$ to convey convergence. If we can, we'll often have a ``candidate'' convergence point $x$ and then check the inequality in the definition. But sometimes we might not have a candidate available. In such a case, the alternative definition if useful.

\begin{definition}
	A sequence  $\{x_n\}_{n=0}^{\infty}$  in $S$ is a \textbf{Cauchy sequence} if for every $\epsilon > 0$, there exists $N_{\epsilon}$ such that 
		\[d(x_n, x_m) < \epsilon \quad \text{ for all } n, m \geq N_{\epsilon}.	\]
	We refer to this condition as satisfying the \textbf{Cauchy criterion}. 
\end{definition}

In other words, the points in the sequence get closer and closer to each other. Here are a few important facts about convergence and Cauchy sequences.

\begin{itemize}
	\itemsep0em
	\item If $x_n \rightarrow x$ and $x_n \rightarrow y$, then $x=y$. In other words, if $\{x_n\}$ has a limit, then it has a unique limit. 
	\item If $\{x_n\}$ is a Cauchy sequence, then it is bounded.
	\item $x_n \rightarrow x$ if and only if every subsequence of $\{x_n\}$ converges to $x$. 
\end{itemize}
The proofs are not difficult and are worth doing if you feel you need a bit of an exercise. But anyway, this so far has all assumed that a limit point actually exists. So, um, when does a limit point actually exist? Read on.

\begin{definition}
	A metric space $(S, d)$ is \textbf{complete} if every Cauchy sequence in $S$ converges to an element in $S$.
\end{definition}

\begin{definition}
	A complete normed vector space is called a \textbf{Banach space}. 
\end{definition}

\begin{theorem}
	Let $X \subseteq \R^{l}$ and let $C(X)$ be the set of bounded continuous functions $f: X \rightarrow \R$ with the $\sup$ norm,
		\[\norm{x} = \sup_{x \in X} \abs{f(x)}.	\]	
	Then $C(X)$ is a complete normed vector space. (Note that if $X$ is compact, then every continuous function is bounded. Otherwise the restriction to bounded functions must be added.)
	
\end{theorem}
\begin{proof}
	Suppose $\{f_n\}$ is a Cauchy sequence of bounded continuous functions. Pick some $x \in X$. Notice that
		\[\abs{f_n(x) - f_m(x)} \leq \sup_{y \in X} \abs{f_n(y) - f_m(y)} =  \norm{f_n - f_m}.	\]
Because $\{f_n\}$ is Cauchy, this means $n,m \geq N_{\epsilon}$ implies $\norm{f_n-f_m} \leq \epsilon$, and thus $\abs{f_n(x) - f_m(x)} \leq \epsilon$. Therefore the sequence of real numbers $\{f_n(x)\}$ is also Cauchy. Because the real numbers are complete, it follows that there is some limit point satisfying $f_n(x) \rightarrow f(x)$. 

So for any $x \in X$, we have that $n \geq N_{\epsilon}$ implies $\abs{f_n(x) - f(x)} < \epsilon$. This includes in particular the $x$ that establishes the supremum, that is,
	\[\norm{f_n - f} =  \sup_{x \in X} \abs{f_n(x) - f(x)} < \epsilon.	\]
Thus $\{f_n\}$ converges to $f$ with respect to the $\sup$ norm. 

For boundedness, notice that there exist some $M >0$ such that $\norm{f_n} \leq M$. It follows that $\abs{f_n(x)} \leq M$ for all $x \in X$. In the limit, we get
	\[\abs{f(x)} = \lim_{n \rightarrow \infty} \abs{f_n (x)} \leq M \quad \text{ for all } x \in X. 	\]
Thus, $f$ is bounded.

Finally, continuity. Take $x \in X$ and some $\epsilon >0$. Choose $k$ so that $\norm{f - f_k} < \epsilon /3$. Since $f_k$ is continuous, we can choose $\delta$ such that
	\[\norm{x - y}_E \leq \delta \quad \text{ implies} \quad \abs{f_k(x) - f_k(y)} < \frac{\epsilon}{3},	\] 
where $\norm{\cdot}_E$ is the Euclidean norm. Using the triangle inequality, we have
\begin{align*}
	\abs{f(x) - f(y)} &= \abs{f(x) - f_k(x) + f_k(x) - f_k(y) + f_k(y) - f(y)} \\
		&\leq \abs{f(x) - f_k(x)} + \abs{f_k(x) - f_k(y)} + \abs{f_k(y) - f(y)} \\
		&< \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} \\
		&=\epsilon.
\end{align*}
We've shown that $\norm{x -y}_E \leq \delta$ implies that $\abs{f(x)- f(y)} \leq \epsilon$, which is precisely the continuity condition. 
\end{proof} 	



\section{Contraction Mapping Theorem}

\begin{definition}
	Let $(S, d)$ be a metric space and $T:S \rightarrow S$ be a function from $S$ to itself. $T$ is said to be a \textbf{contraction mapping} with \textbf{modulus} $\beta$ if for some $\beta \in (0,1)$, we have $d(Tx, Ty) \leq \beta d(x,y)$ for all $x, y \in S$. 
\end{definition}

This definition is relatively straightforward. When we put both $x$ and $y$ through the function $T$, then they are closer than they were before. And what's to stop us from putting $Tx$ and $Ty$ back through $T$ to make then even closer again? What's to stop us from doing an arbitrary or even infinite number of iterations of $T$? Hold onto that thought. 

Consider the example in the closed interval $[a,b]$ with $d(x,y)=\abs{x-y}$. The function $T$ will constitute a contraction if some some $\beta \in (0,1)$, we have
	\[	\frac{\abs{Tx - Ty}}{\abs{x-y}} \leq \beta < 1 \quad \text{ for all } x,y \in S,\; x \neq y.\]	

\begin{theorem}[Contraction Mapping Theorem]
	If $(S,d)$ is a complete metric space and $T:S \rightarrow S$ is a contraction mapping with modulus $\beta$, then 
	\begin{enumerate}
		\item $T$ has exactly one fixed point $v$ in $S$,
		\item For any $v_0 \in S$, we have $d(T^nv_0,v) \leq \beta^n d(v_0, v)$ for $n \in \N$. 
	\end{enumerate}
\end{theorem}

This is a nice result. There is a unique fixed point $v$, and furthermore, if we start from \emph{any} $v_0$, then we can take ``enough'' contractions of $v_0$ to get arbitrarily close to the fixed point $v$. Let's prove it!

\begin{proof} I will prove the claims in order.
\begin{enumerate}
	\item	Let's start by defining iterates of $T$ as follows: $T^0x = x$ and $T^nx = T(T^{n-1}x)$. Now let's pick some arbitrary $v_0$ as the ``initial guess'' for $v$. Define the sequence $\{v_n\}_{n=0}^{\infty}$ where $v_{n+1}=Tv_n$, that is,
		\[\{v_n\}_{n=0}^{\infty} = \{v_0, Tv_0, Tv_1, Tv_2, \hdots \}.	\]
	Because $T$ is supposed to be a contraction mapping, it follows that
		\[d(v_2, v_1) = d(Tv_1, Tv_0) \leq \beta d(v_1,v_0).	\]
	This can be extrapolated via induction so that
\begin{equation}
	d(v_{n+1}, v_n) \leq \beta^n d(v_1,v_0) \quad \text{ for any } n \in \N.	 \label{inductioncmap}
\end{equation}
	Now consider some $m > n$. It follows that
	\begin{align}
		d(v_m, v_n) &\leq d(v_m, v_{m-1}) + \hdots + d(v_{n+1}, v_n) \label{trieqcm}\\
	&\leq \left(\beta^{m-1} + \beta^{m-2} + \hdots + \beta^n	\right) d(v_1, v_0) \label{leqinduction} \\
	&= \beta^n \left(\beta^{m-n-1} + \beta^{m-n-2} + \hdots + \beta + 1	\right) d(v_1, v_0) \nonumber \\
	& =  \frac{\beta^n ( 1 - \beta^{m-n})}{1 - \beta}d(v_1, v_0) \nonumber \\
	& \leq \frac{\beta^n}{1 - \beta}d(v_1, v_0). \nonumber
	\end{align}
Line (\ref{trieqcm}) follows from the triangle inequality, and line (\ref{leqinduction}) follows from equation (\ref{inductioncmap}). 

The implication is that $\{v_n\}$ forms a Cauchy sequence -- the last line is the $\epsilon$ term and $N_{\epsilon}$ is $n$. For instance, if we choose 
	\[\epsilon = \frac{\beta^5}{1 - \beta}d(v_1, v_0),	\]
then it follows that $d(v_m, v_n) \leq \epsilon$ for any $m, n \geq 5$. Since $S$ is complete by supposition, it follows that $v_n \rightarrow v \in S$. 

Now we want to show that there exists exactly one fixed point. Via the triangle inequality, we have
	\begin{align*}
		d(Tv, v) &\leq d(Tv, T^nv_0) + d(T^n v_0, v) \\
			&\leq \beta d(v, T^{n-1}v_0) + d(T^n v_0, v) \\
			&=  \beta d(v, v_{n-1}) + d(v_n, v).
	\end{align*}
We have already shown that $v_n \rightarrow v$, so the two right-most terms converge to zero. Thus, $d(Tv,v) \rightarrow 0$, that is, $Tv=v$. 

What remains is to show that $v$ is a unique fixed point. Suppose there exists some other fixed point $v'$. So $Tv=v$ and $Tv'=v'$. Then it must be the case that  $d(v', v) = d(Tv', Tv) \leq \beta d(v', v)$. Since $\beta < 1$, this cannot hold. Thus, the fixed point $v$ must be unique.

 \item Oh yeah, there's another part to this proof. Thankfully it won't take nearly as much work. Notice that for any $n \geq 1$, we have
	\[d(T^nv_0,v) = d\big( T(T^{n-1}v_0), Tv\big)  \leq \beta d(T^{n-1}v_o, v).	\]
	So simply repeat this via induction and the result follows. \qedhere
\end{enumerate}
\end{proof} 

\begin{theorem}[Blackwell's Sufficient Conditions]
Suppose $X \subseteq \R^l$ and $B(X)$ is a space of bounded functions $f:X \rightarrow \R$ with the $\sup$ norm. Let $T: B(X) \rightarrow B(X)$ be an operator satisfying the following two properties:
\begin{itemize}
	\item \emph{Monotonicity}. If $f, g\in  B(X)$ and $f(x) \leq g(x)$ for all $x \in X$, then $(Tf)(x) \leq (Tg)(x)$ for all $x \in X$;
	\item \emph{Discounting}. There exists some $\beta \in (0,1)$ such that
		\[	T(f + a)(x) \leq (Tf)(x) + \beta a \quad \text{ for all } f \in B(X), \alpha \geq 0, x\in X.\]
		(Note that $(f+a)(x) = f(x) + a$.)
\end{itemize}
Then $T$ is a contraction with modulus $\beta$. 
\end{theorem}
\begin{proof}
	Suppose that $f(x) \leq g(x)$ for all $x \in X$. Then $f \leq g$. Because $\norm{f-g}$ is nonnegative, and since $f \leq g$, it follows that $f \leq g + \norm{f-g}$. Letting $a = \norm{f-g}$ apropos the discounting property, we have
	\[	T(g + \norm{f-g}) \leq Tg + \beta \norm{f-g}.	\]
So by monotonicity, we have
	\[	Tf \leq Tg \leq Tg + \beta \norm{f-g}. \]
Thus, we have $Tf - Tg \leq \beta \norm{f-g}$, which implies $Tf(x) - Tg(x) \leq  \beta \norm{f-g}$ for all $x \in X$.

Now consider the case where $g(x) \leq f(x)$ for all $x \in X$. Follow the same logic to achieve the result $Tg(x) - Tf(x) \leq  \beta \norm{f-g}$ for all $x \in X$. Multiplying both sides by $-1$ gives
	\[ 	-\beta \norm{f-g} \leq Tf(x)-Tg(x).\]
So when considering both cases $f \leq g$ and $g \leq f$, we have for all $x \in X$ that
	\[-\beta \norm{f-g} \leq Tf(x)-Tg(x)\leq \beta \norm{f-g}, \]
which implies that $\abs{Tf(x) - Tg(x)} \leq \beta \norm{f-g}$. This is true in particular for the $x \in X$ that establishes the supremum, so we can say
	\[ \norm{Tf - Tg} \leq \beta \norm{f-g}.	\qedhere \]
\end{proof}

The hypothesis of Blackwell's theorem can often be verified rather easily and quickly. Consider the operator $T$ from the method of successive approximations,
	\[	(Tv)(k) = \max_{0 \leq k' \leq f(k)} \left\{ U\big( f(k) - k' \big) + \beta v(k')\right\}.\]	
	If there is some $w(\cdot)$ such that $v(k') \leq w(k')$ for all $k'$, then maximized objective function for $Tw$ is uniformly higher than that for $Tv$. That is, 
\begin{align*}
	(Tw)(k) &= \max_{0 \leq k' \leq f(k)} \left\{ U\big( f(k) - k' \big) + \beta w(k')\right\} \\
		&\geq \max_{0 \leq k' \leq f(k)} \left\{ U\big( f(k) - k' \big) + \beta v(k')\right\} \\
		&= (Tv)(k).
\end{align*}
Thus, monotonicity holds.

Discounting is also rather straightforward:
\begin{align*}
	T(v+a)(k) &= \max_{0 \leq k' \leq f(k)} \left\{ U\big( f(k) - k' \big) + \beta [v(k') + a]\right\} \\
		&= \max_{0 \leq k' \leq f(k)} \left\{ U\big( f(k) - k' \big) + \beta [v(k')]\right\} + \beta a\\
		&= (Tv)(k) + \beta a.
\end{align*}
So in such a case, we could apply the contraction mapping theorem to our method of successive approximations.

Indeed, perhaps foreshadowing a bit, you might be able to see why this could be useful insofar as the method of successive approximations could allow us to contract our way to a solution. Make a guess, any garbage guess at all (although zero seems like the most ``natural'' guess), and continually apply a contraction mapping until it ``settles down'' to the maximizing function. 

	
\end{document}
 
 	
